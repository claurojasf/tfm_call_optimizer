{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from pinecone import Pinecone  # ✅ Correct import for Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Hugging Face API Key (Ensure it's set in your environment variables)\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not HUGGINGFACE_API_KEY:\n",
    "    raise ValueError(\"❌ Missing Hugging Face API Key. Set it as HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# ✅ Load DeepSeek-R1 via API Inference\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",  # Model name\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 2048},\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# ✅ Initialize Pinecone Client\n",
    "pinecone_client = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# ✅ Ensure index name is set\n",
    "index_name = os.getenv(\"PINECONE_INDEX\")\n",
    "if not index_name:\n",
    "    raise ValueError(\"❌ PINECONE_INDEX is not set. Check your .env file.\")\n",
    "\n",
    "# ✅ Ensure the index exists before using it\n",
    "existing_indexes = [idx[\"name\"] for idx in pinecone_client.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    raise ValueError(f\"❌ Index '{index_name}' does not exist in Pinecone. Please create it first.\")\n",
    "\n",
    "# ✅ Initialize Pinecone Index\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# ✅ Initialize the same embedding model used for storing embeddings\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# ✅ Load the vector store for retrieval\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,  # ✅ Pass the initialized Pinecone index\n",
    "    embedding=huggingface_embeddings,\n",
    "    text_key=\"text\"  # ✅ Specify the key containing stored text\n",
    ")\n",
    "\n",
    "# ✅ Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  \n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "# ✅ Define the Prompt Template for RAG-based Q&A\n",
    "prompt_template = \"\"\"\n",
    "You are a auditor expert in analyzing call center transcription calls.\n",
    "\n",
    "Instructions:\n",
    "1. Answer based only on the context (delimited by <ctx> </ctx>).\n",
    "2. If the answer is not in the context, respond with \"I don't have this information.\"\n",
    "3. Provide only the relevant answer to the user's question.\n",
    "\n",
    "-------\n",
    "<ctx>\n",
    "{context}\n",
    "</ctx>\n",
    "-------\n",
    "\n",
    "Question: {question}\n",
    "Useful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ✅ Memory buffer to handle multiple interactions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "# ✅ Define Retrieval-Based Q&A Chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"memory\": memory}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the remaining balance in call_004?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Which calls mention the word 'extensions'?\"\n",
    "#query = \"Which calls mention  the word 'payment plan'?\"\n",
    "query = \"Which calls mention the word 'confirmation'?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"is there any call that talk about loosing a job? give me the specific call ID\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"give me a summary from the call_004\"\n",
    "query = \"give me a summary from the call_003\"\n",
    "\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what call did John call?\"\n",
    "\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
