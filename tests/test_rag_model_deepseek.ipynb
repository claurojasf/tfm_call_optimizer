{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_11492\\2885508344.py:18: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ DeepSeek-R1 Response:\n",
      "tell me about DeeepSeek. DeepSeek is a large language model developed by DeepSeek AI, designed to understand and generate human-like text based on the input it receives. Here are some key points about DeepSeek:\n",
      "\n",
      "1. **Size**: DeepSeek is a large model, with the most recent version being DeepSeek Coder 33B, which has 33 billion parameters. This size allows it to understand and generate complex text with a high degree of accuracy.\n",
      "\n",
      "2. **Training Data**: DeepSeek is trained on a diverse range of internet text data, which enables it to generate responses on a wide variety of topics. It's also trained to understand and generate code, making it useful for tasks like programming assistance.\n",
      "\n",
      "3. **Capabilities**: DeepSeek can perform tasks such as text generation, translation, summarization, question answering, and more. It can also help with coding tasks like code completion, debugging, and explaining code.\n",
      "\n",
      "4. **Limitations**: While DeepSeek is powerful, it's important to remember that it's a machine learning model and not a human. It doesn't have personal experiences, feelings, or real-time knowledge. It generates responses based on the data it was trained on, which can sometimes lead to incorrect or misleading outputs.\n",
      "\n",
      "5. **Use Cases**: DeepSeek can be used in various applications, such as chatbots, virtual assistants, content generation, and AI-powered coding tools.\n",
      "\n",
      "6. **Development**: DeepSeek AI is continuously working on improving and updating the model. They've released several versions of DeepSeek, each with improvements over the previous one.\n",
      "\n",
      "7. **Access**: DeepSeek is available for use through DeepSeek AI's API. They also provide a web interface for interacting with the model.\n",
      "\n",
      "Here are some resources to learn more about DeepSeek:\n",
      "\n",
      "- DeepSeek AI's official website: <https://deepseek.ai/>\n",
      "- DeepSeek GitHub repository: <https://github.com/DeepSeek-AI/DeepSeek>\n",
      "- DeepSeek API documentation: <https://docs.deepseek.ai/>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚úÖ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ Hugging Face API Key (Ensure it's set in your environment variables)\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "if not HUGGINGFACE_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing Hugging Face API Key. Set it as HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "# ‚úÖ Load DeepSeek-R1 via API Inference\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",  # Model name\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 2048},\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"tell me about DeeepSeek.\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(\"\\nüîπ DeepSeek-R1 Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you show me the top 10 best movies of all time? Here are the top 10 movies as ranked by IMDb as of 2021:\n",
      "\n",
      "1. **The Godfather** (1972) - Crime, Drama\n",
      "   - Director: Francis Ford Coppola\n",
      "   - Stars: Marlon Brando, Al Pacino, James Caan\n",
      "\n",
      "2. **The Dark Knight** (2008) - Action, Crime, Drama\n",
      "   - Director: Christopher Nolan\n",
      "   - Stars: Christian Bale, Heath Ledger, Aaron Eckhart\n",
      "\n",
      "3. **12 Angry Men** (1957) - Drama\n",
      "   - Director: Sidney Lumet\n",
      "   - Stars: Henry Fonda, Lee J. Cobb, Martin Balsam\n",
      "\n",
      "4. **Schindler's List** (1993) - Biography, Drama, History\n",
      "   - Director: Steven Spielberg\n",
      "   - Stars: Liam Neeson, Ralph Fiennes, Ben Kingsley\n",
      "\n",
      "5. **Pulp Fiction** (1994) - Crime, Drama\n",
      "   - Director: Quentin Tarantino\n",
      "   - Stars: John Travolta, Uma Thurman, Samuel L. Jackson\n",
      "\n",
      "6. **The Lord of the Rings: The Return of the King** (2003) - Adventure, Drama, Fantasy\n",
      "   - Director: Peter Jackson\n",
      "   - Stars: Elijah Wood, Viggo Mortensen, Ian McKellen\n",
      "\n",
      "7. **The Good, the Bad and the Ugly** (1966) - Western\n",
      "   - Director: Sergio Leone\n",
      "   - Stars: Clint Eastwood, Lee Van Cleef, Eli Wallach\n",
      "\n",
      "8. **Fight Club** (1999) - Drama\n",
      "   - Director: David Fincher\n",
      "   - Stars: Brad Pitt, Edward Norton, Meat Loaf\n",
      "\n",
      "9. **Forrest Gump** (1994) - Comedy, Drama, Romance\n",
      "   - Director: Robert Zemeckis\n",
      "   - Stars: Tom Hanks, Robin Wright, Gary Sinise\n",
      "\n",
      "10. **Star Wars: Episode V - The Empire Strikes Back** (1980) - Action, Adventure, Fantasy\n",
      "    - Director: Irvin Kershner\n",
      "    - Stars: Mark Hamill, Harrison Ford, Carrie Fisher\n",
      "\n",
      "These movies span various genres and decades, showcasing the diversity and richness of cinema.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you show me the top 10 best movies of all time?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you name the top 10 soccer players in history?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"give me a short summary of the little prince book.\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"how can i learn python quikly?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"How does DeepSeek-R1 differ from Mistral-7B?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you give me an example of prompting structure for DeepSeek R-1 model? How can I ask it to generate a detailed report about a specific topic?\n",
      "\n",
      "In this case, let's consider the topic: \"The Impact of Climate Change on Global Agriculture\"\n",
      "\n",
      "Here's an example of how you can structure a prompt for the DeepSeek R-1 model to generate a detailed report on the topic:\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt:**\n",
      "\n",
      "\"Write a comprehensive and detailed report on 'The Impact of Climate Change on Global Agriculture'. Ensure the report includes the following sections and aspects:\n",
      "\n",
      "1. **Introduction**\n",
      "   - Brief overview of climate change and its causes\n",
      "   - Importance of agriculture and its global significance\n",
      "\n",
      "2. **Current State of Global Agriculture**\n",
      "   - Global agricultural production and its trends\n",
      "   - Major crops and their geographical distribution\n",
      "   - Existing challenges in global agriculture\n",
      "\n",
      "3. **Impacts of Climate Change on Agriculture**\n",
      "   - **Temperature Rise**\n",
      "     - Effects on crop yields and livestock\n",
      "     - Changes in growing seasons and crop calendars\n",
      "   - **Precipitation Patterns**\n",
      "     - Increased frequency of droughts and floods\n",
      "     - Effects on irrigation and water stress\n",
      "   - **Rising Sea Levels**\n",
      "     - Loss of agricultural land in coastal areas\n",
      "     - Salinization of soil and water resources\n",
      "   - **Increased Frequency of Extreme Weather Events**\n",
      "     - Impacts on agricultural infrastructure and crops\n",
      "     - Economic losses and food security implications\n",
      "\n",
      "4. **Regional Impacts**\n",
      "   - Detailed analysis of how climate change affects agriculture in different regions (e.g., Asia, Africa, Europe, North America, South America, and Oceania)\n",
      "   - Unique challenges and vulnerabilities faced by each region\n",
      "\n",
      "5. **Consequences for Food Security and Economy**\n",
      "   - Global food production trends and projections\n",
      "   - Impacts on food prices, access, and availability\n",
      "   - Economic losses and job displacement in the agriculture sector\n",
      "\n",
      "6. **Adaptation Measures and Mitigation Strategies**\n",
      "   - Crop diversification and improved crop management practices\n",
      "   - Climate-smart agriculture and sustainable intensification\n",
      "   - Role of technology and innovation in climate-resilient agriculture\n",
      "   - International cooperation and policy interventions\n",
      "\n",
      "7. **Conclusion**\n",
      "   - Summary of key findings and implications\n",
      "   - Call to action for policymakers, scientists, and the public\n",
      "\n",
      "Please ensure the report is well-structured, informative, and backed by credible sources. Cite relevant studies, reports, and data to support the information presented. Use clear and concise language, making complex concepts accessible to a broad audience.\n",
      "\n",
      "**Format:** Please format the report as a bullet-pointed list under each section heading for easy navigation and understanding.\n",
      "\n",
      "**Length:** Aim for a detailed report of around 1500-2000 words.\"\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you give me an example of prompting structure for DeepSeek R-1 model?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
