{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_17868\\2885508344.py:18: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ DeepSeek-R1 Response:\n",
      "tell me about DeeepSeek.  What is it?  How does it work?  What does it do?  What are its benefits?\n",
      "\n",
      "DeepSeek is an open-source AI assistant developed by the DeepSeek AI organization. It's designed to provide helpful, respectful, and honest interactions with users, offering a range of capabilities such as answering questions, generating text, and solving problems. Here's a breakdown of what it is, how it works, its benefits, and more:\n",
      "\n",
      "1. **What is DeepSeek?**\n",
      "   DeepSeek is a large language model AI assistant built using the latest transformer-based models. It's trained on a vast amount of data from the internet, allowing it to understand and generate human-like text for a wide range of tasks.\n",
      "\n",
      "2. **How does it work?**\n",
      "   - **Prompt Engineering**: DeepSeek generates responses based on the input (or prompt) it receives. Users can guide the AI's response by crafting specific prompts.\n",
      "   - **Context Window**: The model maintains a context window, remembering previous parts of the conversation to provide more coherent and relevant responses.\n",
      "   - **Probabilistic Generation**: DeepSeek generates responses by predicting the most likely next tokens (words or subwords) based on the input prompt and its internal model.\n",
      "   - **Fine-tuning and Reinforcement Learning from Human Feedback (RLHF)**: DeepSeek has been fine-tuned and trained using RLHF to better align with human preferences, making its responses more helpful, honest, and harmless.\n",
      "\n",
      "3. **What does it do?**\n",
      "   - **Answer Questions**: DeepSeek can provide information on a wide range of topics.\n",
      "   - **Generate Text**: It can create stories, poems, articles, or any other type of text based on a given prompt.\n",
      "   - **Solve Problems**: DeepSeek can help with tasks like solving math problems, explaining complex concepts, or providing step-by-step instructions.\n",
      "   - **Engage in Dialogue**: It can participate in conversations on various topics, providing insights and engaging in discussion.\n",
      "\n",
      "4. **Benefits of using DeepSeek:**\n",
      "   - **Access to Information**: DeepSeek can provide quick and convenient access to information on numerous topics.\n",
      "   - **Learning and Education**: It can help explain complex concepts, making learning more accessible.\n",
      "   - **Creativity**: DeepSeek can generate creative content, such as stories, poems, or song lyrics, based on user prompts.\n",
      "   - **Problem-solving**: It can assist with solving problems, providing step-by-step guidance, or explaining solutions.\n",
      "   - **Open-source and Customizable**: As an open-source project, DeepSeek can be customized, fine-tuned, or integrated into other applications.\n",
      "\n",
      "5. **Limitations and considerations:**\n",
      "   - **Hallucinations**: DeepSeek can sometimes provide incorrect or misleading information, a phenomenon known as \"hallucination.\"\n",
      "   - **Lack of Real-time Information**: The model's knowledge cutoff is 2021, so it might not have real-time or up-to-date information.\n",
      "   - **Bias and Limitations of Training Data**: DeepSeek's responses may reflect biases present in its training data, and it might struggle with tasks that require real-world understanding or physical manipulation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚úÖ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ Hugging Face API Key (Ensure it's set in your environment variables)\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "if not HUGGINGFACE_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing Hugging Face API Key. Set it as HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "# ‚úÖ Load DeepSeek-R1 via API Inference\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",  # Model name\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 2048},\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"tell me about DeeepSeek.\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(\"\\nüîπ DeepSeek-R1 Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you show me the top 10 best movies of all time? Here are the top 10 movies as ranked by IMDb as of 2021:\n",
      "\n",
      "1. **The Godfather** (1972) - Crime, Drama\n",
      "   - Director: Francis Ford Coppola\n",
      "   - Stars: Marlon Brando, Al Pacino, James Caan\n",
      "\n",
      "2. **The Dark Knight** (2008) - Action, Crime, Drama\n",
      "   - Director: Christopher Nolan\n",
      "   - Stars: Christian Bale, Heath Ledger, Aaron Eckhart\n",
      "\n",
      "3. **12 Angry Men** (1957) - Drama\n",
      "   - Director: Sidney Lumet\n",
      "   - Stars: Henry Fonda, Lee J. Cobb, Martin Balsam\n",
      "\n",
      "4. **Schindler's List** (1993) - Biography, Drama, History\n",
      "   - Director: Steven Spielberg\n",
      "   - Stars: Liam Neeson, Ralph Fiennes, Ben Kingsley\n",
      "\n",
      "5. **Pulp Fiction** (1994) - Crime, Drama\n",
      "   - Director: Quentin Tarantino\n",
      "   - Stars: John Travolta, Uma Thurman, Samuel L. Jackson\n",
      "\n",
      "6. **The Lord of the Rings: The Return of the King** (2003) - Adventure, Drama, Fantasy\n",
      "   - Director: Peter Jackson\n",
      "   - Stars: Elijah Wood, Viggo Mortensen, Ian McKellen\n",
      "\n",
      "7. **The Good, the Bad and the Ugly** (1966) - Western\n",
      "   - Director: Sergio Leone\n",
      "   - Stars: Clint Eastwood, Lee Van Cleef, Eli Wallach\n",
      "\n",
      "8. **Fight Club** (1999) - Drama\n",
      "   - Director: David Fincher\n",
      "   - Stars: Brad Pitt, Edward Norton, Meat Loaf\n",
      "\n",
      "9. **Forrest Gump** (1994) - Comedy, Drama, Romance\n",
      "   - Director: Robert Zemeckis\n",
      "   - Stars: Tom Hanks, Robin Wright, Gary Sinise\n",
      "\n",
      "10. **Star Wars: Episode V - The Empire Strikes Back** (1980) - Action, Adventure, Fantasy\n",
      "    - Director: Irvin Kershner\n",
      "    - Stars: Mark Hamill, Harrison Ford, Carrie Fisher\n",
      "\n",
      "These movies span various genres and decades, showcasing the diversity and richness of cinema.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you show me the top 10 best movies of all time?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you name the top 10 soccer players in history?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"give me a short summary of the little prince book.\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"how can i learn python quikly?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"How does DeepSeek-R1 differ from Mistral-7B?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you give me an example of prompting structure for DeepSeek R-1 model? How can I ask it to generate a detailed report about a specific topic?\n",
      "\n",
      "In this case, let's consider the topic: \"The Impact of Climate Change on Global Agriculture\"\n",
      "\n",
      "Here's an example of how you can structure a prompt for the DeepSeek R-1 model to generate a detailed report on the topic:\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt:**\n",
      "\n",
      "\"Write a comprehensive and detailed report on 'The Impact of Climate Change on Global Agriculture'. Ensure the report includes the following sections and aspects:\n",
      "\n",
      "1. **Introduction**\n",
      "   - Brief overview of climate change and its causes\n",
      "   - Importance of agriculture and its global significance\n",
      "\n",
      "2. **Current State of Global Agriculture**\n",
      "   - Global agricultural production and its trends\n",
      "   - Major crops and their geographical distribution\n",
      "   - Existing challenges in global agriculture\n",
      "\n",
      "3. **Impacts of Climate Change on Agriculture**\n",
      "   - **Temperature Rise**\n",
      "     - Effects on crop yields and livestock\n",
      "     - Changes in growing seasons and crop calendars\n",
      "   - **Precipitation Patterns**\n",
      "     - Increased frequency of droughts and floods\n",
      "     - Effects on irrigation and water stress\n",
      "   - **Rising Sea Levels**\n",
      "     - Loss of agricultural land in coastal areas\n",
      "     - Salinization of soil and water resources\n",
      "   - **Increased Frequency of Extreme Weather Events**\n",
      "     - Impacts on agricultural infrastructure and crops\n",
      "     - Economic losses and food security implications\n",
      "\n",
      "4. **Regional Impacts**\n",
      "   - Detailed analysis of how climate change affects agriculture in different regions (e.g., Asia, Africa, Europe, North America, South America, and Oceania)\n",
      "   - Unique challenges and vulnerabilities faced by each region\n",
      "\n",
      "5. **Consequences for Food Security and Economy**\n",
      "   - Global food production trends and projections\n",
      "   - Impacts on food prices, access, and availability\n",
      "   - Economic losses and job displacement in the agriculture sector\n",
      "\n",
      "6. **Adaptation Measures and Mitigation Strategies**\n",
      "   - Crop diversification and improved crop management practices\n",
      "   - Climate-smart agriculture and sustainable intensification\n",
      "   - Role of technology and innovation in climate-resilient agriculture\n",
      "   - International cooperation and policy interventions\n",
      "\n",
      "7. **Conclusion**\n",
      "   - Summary of key findings and implications\n",
      "   - Call to action for policymakers, scientists, and the public\n",
      "\n",
      "Please ensure the report is well-structured, informative, and backed by credible sources. Cite relevant studies, reports, and data to support the information presented. Use clear and concise language, making complex concepts accessible to a broad audience.\n",
      "\n",
      "**Format:** Please format the report as a bullet-pointed list under each section heading for easy navigation and understanding.\n",
      "\n",
      "**Length:** Aim for a detailed report of around 1500-2000 words.\"\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example Query (Question Answering)\n",
    "query = \"can you give me an example of prompting structure for DeepSeek R-1 model?\"\n",
    "response = llm.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me only the useful answer: '\n",
      "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
      "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
      "You only need to show me the useful answer the rest of the template please dont print it\n",
      "\n",
      "\n",
      "Instructions:\n",
      "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
      "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
      "3. Provide a concise and precise answer.\n",
      "4. Quiero las respuestas en idioma espa√±ol.\n",
      "\n",
      "Provided Information\n",
      "-------\n",
      "<ctx>\n",
      "Context: S√≠, en una nueva llamada. ¬øSabe a Amor a Salles? ¬ø que Natalia convierte en un gusto? Natalia de Unas Salles con Daniela Rojas. Yo soy una autopat√≠a comunicada. Muy bien, estoy Daniela\n",
      "\n",
      "59  \n",
      " \n",
      "‚Ä¢ --archive - Re-indexa las listas archivadas. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vicidial - Procesos de Backup \n",
      "   \n",
      "    \n",
      "          \n",
      "Ir a la navegaci√≥n   Ir a la b√∫squeda\n",
      "\n",
      "bases de datos que no son parte de la instalaci√≥n est√°ndar.  \n",
      "GRABACIONES DE LLAMADAS \n",
      "Puede consultar el documento Vicidial - Archivado de Grabaciones  para obtener \n",
      "informaci√≥n detallado sobre el manejo de Archivos de Grabaciones de Llamadas.  \n",
      "Las grabaciones se almacenan en el filesystem /var/spool/recordings de cada\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "Chat History: \n",
      "</hs>\n",
      "-------\n",
      "Question: dame un resumen de la llamada\n",
      "\n",
      "Useful Answer:\n",
      "La llamada comienza con una presentaci√≥n de la asesora Natalia de Unas Salles y la cliente Daniela Rojas. Se menciona que la asesora es una \"autopat√≠a comunicada\". Luego, se escucha una breve descripci√≥n de un proceso de backup relacionado con Vicidial y las grabaciones de llamadas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"give me only the useful answer: '\\nYou are an expert auditor analyzing call center transcription calls from custumer support calls.\\nYou need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\\nYou only need to show me the useful answer the rest of the template please dont print it\\n\\n\\nInstructions:\\n1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\\n2. If the information is not in the context, respond: \"I don\\'t have this information.\"\\n3. Provide a concise and precise answer.\\n4. Quiero las respuestas en idioma espa√±ol.\\n\\nProvided Information\\n-------\\n<ctx>\\nContext: S√≠, en una nueva llamada. ¬øSabe a Amor a Salles? ¬ø que Natalia convierte en un gusto? Natalia de Unas Salles con Daniela Rojas. Yo soy una autopat√≠a comunicada. Muy bien, estoy Daniela\\n\\n59  \\n \\n‚Ä¢ --archive - Re-indexa las listas archivadas. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nVicidial - Procesos de Backup \\n   \\n    \\n          \\nIr a la navegaci√≥n   Ir a la b√∫squeda\\n\\nbases de datos que no son parte de la instalaci√≥n est√°ndar.  \\nGRABACIONES DE LLAMADAS \\nPuede consultar el documento Vicidial - Archivado de Grabaciones  para obtener \\ninformaci√≥n detallado sobre el manejo de Archivos de Grabaciones de Llamadas.  \\nLas grabaciones se almacenan en el filesystem /var/spool/recordings de cada\\n</ctx>\\n-------\\n<hs>\\nChat History: \\n</hs>\\n-------\\nQuestion: dame un resumen de la llamada\\n\\nUseful Answer:\\nLa llamada comienza con una presentaci√≥n de la asesora Natalia de Unas Salles y la cliente Daniela Rojas. Se menciona que la asesora es una \"autopat√≠a comunicada\". Luego, se escucha una breve descripci√≥n de un proceso de backup relacionado con Vicidial y las grabaciones de llamadas.\"\"\"\n",
    "response = llm(query)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me only the useful answer: \\'\\nYou are an expert auditor analyzing call center transcription calls from custumer support calls.\\nYou need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\\nYou only need to show me the useful answer the rest of the template please dont print it\\n\\n\\nInstructions:\\n1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\\n2. If the information is not in the context, respond: \"I don\\'t have this information.\"\\n3. Provide a concise and precise answer.\\n4. Quiero las respuestas en idioma espa√±ol.\\n\\nProvided Information\\n-------\\n<ctx>\\nContext: S√≠, en una nueva llamada. ¬øSabe a Amor a Salles? ¬ø que Natalia convierte en un gusto? Natalia de Unas Salles con Daniela Rojas. Yo soy una autopat√≠a comunicada. Muy bien, estoy Daniela\\n\\n59  \\n \\n‚Ä¢ --archive - Re-indexa las listas archivadas. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nVicidial - Procesos de Backup \\n   \\n    \\n          \\nIr a la navegaci√≥n   Ir a la b√∫squeda\\n\\nbases de datos que no son parte de la instalaci√≥n est√°ndar.  \\nGRABACIONES DE LLAMADAS \\nPuede consultar el documento Vicidial - Archivado de Grabaciones  para obtener \\ninformaci√≥n detallado sobre el manejo de Archivos de Grabaciones de Llamadas.  \\nLas grabaciones se almacenan en el filesystem /var/spool/recordings de cada\\n</ctx>\\n-------\\n<hs>\\nChat History: \\n</hs>\\n-------\\nQuestion: dame un resumen de la llamada\\n\\nUseful Answer:\\nLa llamada comienza con una presentaci√≥n de la asesora Natalia de Unas Salles y la cliente Daniela Rojas. Se menciona que la asesora es una \"autopat√≠a comunicada\". Luego, se escucha una breve descripci√≥n de un proceso de backup relacionado con Vicidial y las grabaciones de llamadas.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
