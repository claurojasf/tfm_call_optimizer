{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_20380\\3906497703.py:20: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_20380\\3906497703.py:43: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_20380\\3906497703.py:50: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
      "  vectorstore = PineconeVectorStore(\n",
      "C:\\Users\\Claudio\\AppData\\Local\\Temp\\ipykernel_20380\\3906497703.py:108: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from pinecone import Pinecone  # ✅ Correct import for Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Hugging Face API Key (Ensure it's set in your environment variables)\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not HUGGINGFACE_API_KEY:\n",
    "    raise ValueError(\"❌ Missing Hugging Face API Key. Set it as HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# ✅ Load DeepSeek-R1 via API Inference\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",  # Model name\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 2048},\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# ✅ Initialize Pinecone Client\n",
    "pinecone_client = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# ✅ Ensure the new Pinecone index name is set\n",
    "index_name = os.getenv(\"PINECONE_INDEX_V2\")  # ✅ Updated to use the latest index\n",
    "if not index_name:\n",
    "    raise ValueError(\"❌ PINECONE_INDEX_V2 is not set. Check your .env file.\")\n",
    "\n",
    "# ✅ Ensure the index exists before using it\n",
    "existing_indexes = [idx[\"name\"] for idx in pinecone_client.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    raise ValueError(f\"❌ Index '{index_name}' does not exist in Pinecone. Please create it first.\")\n",
    "\n",
    "# ✅ Initialize Pinecone Index\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# ✅ Initialize the same embedding model used for storing embeddings\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# ✅ Load the vector store for retrieval\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,  # ✅ Connects to the updated Pinecone index\n",
    "    embedding=huggingface_embeddings,\n",
    "    text_key=\"text\"  # ✅ Specifies where text metadata is stored\n",
    ")\n",
    "\n",
    "# ✅ Define the specific call ID for testing\n",
    "test_call_id = \"call_002\"  # Change this to test different calls\n",
    "\n",
    "# ✅ Create a retriever for call_002 + general PDF solutions\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"filter\": {\n",
    "            \"$or\": [\n",
    "                {\"call_id\": test_call_id},\n",
    "                {\"source\": \"pdf\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Define the Prompt Template for RAG-based Q&A\n",
    "prompt_template = \"\"\"\n",
    "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
    "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
    "\n",
    "Instructions:\n",
    "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
    "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
    "3. Provide a concise and precise answer.\n",
    "\n",
    "Provided Information\n",
    "-------\n",
    "<ctx>\n",
    "Context: {context}\n",
    "</ctx>\n",
    "-------\n",
    "<hs>\n",
    "Chat History: {chat_history}\n",
    "</hs>\n",
    "-------\n",
    "Question: {question}\n",
    "\n",
    "Useful Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"]  # ✅ Includes chat history now\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Memory buffer to handle multiple interactions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "# ✅ Define Retrieval-Based Q&A Chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"memory\": memory}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear() # para resetear memoria (si no se hace, se puede superar el limite de tokens y da error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
      "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
      "\n",
      "Instructions:\n",
      "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
      "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
      "3. Provide a concise and precise answer.\n",
      "\n",
      "Provided Information\n",
      "-------\n",
      "<ctx>\n",
      "Context: Client: It would, but I don’t know if I’ll have a job by then.\n",
      "Agent: We also offer hardship plans that extend payments over 12 months with no penalties.\n",
      "Client: That sounds better. What would the payments look like?\n",
      "Agent: It would be about $30 per month for 12 months.\n",
      "Client: Okay, but what happens if I miss a payment?\n",
      "\n",
      "Agent: Good afternoon, this is John from Recovery Services. I see your balance of $350 is overdue.\n",
      "Client: Yes, I just lost my job and can’t make a payment right now.\n",
      "Agent: I’m very sorry to hear that. Would a one-month extension help?\n",
      "Client: It would, but I don’t know if I’ll have a job by then.\n",
      "\n",
      "Client: Okay, but what happens if I miss a payment?\n",
      "Agent: If you notify us ahead of time, we can reschedule without late fees.\n",
      "Client: That’s really helpful. Will this affect my credit score?\n",
      "Agent: No, as long as you follow the agreement.\n",
      "Client: I’d like to do that, then.\n",
      "Agent: Great! I’ll send you a hardship agreement for signature today.\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "Chat History: \n",
      "</hs>\n",
      "-------\n",
      "Question: how much is the amount per month the client would need to pay?\n",
      "\n",
      "Useful Answer:\n",
      "The client would need to pay $30 per month for 12 months.\n"
     ]
    }
   ],
   "source": [
    "query = \"how much is the amount per month the client would need to pay?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
      "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
      "\n",
      "Instructions:\n",
      "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
      "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
      "3. Provide a concise and precise answer.\n",
      "\n",
      "Provided Information\n",
      "-------\n",
      "<ctx>\n",
      "Context: de Asterisk.  \n",
      "VOICEM\n",
      "AILBAC\n",
      "KUP  \n",
      "NA  \n",
      "Acti\n",
      "vad\n",
      "o \n",
      "por \n",
      "Def\n",
      "ecto  \n",
      "NA  \n",
      "vicidia\n",
      "l_app.\n",
      "tgz  \n",
      "TAR \n",
      "Comp\n",
      "rimid\n",
      "o  \n",
      "Backup de los archivos de la \n",
      "aplicación de Call Center \n",
      "Vicidial.  \n",
      "VICIDIA\n",
      "LBACKU\n",
      "P  \n",
      "Activ\n",
      "ado \n",
      "por \n",
      "Defe\n",
      "cto  \n",
      "Acti\n",
      "vad\n",
      "o \n",
      "por \n",
      "Def\n",
      "ecto  \n",
      "Acti\n",
      "vad\n",
      "o \n",
      "por \n",
      "Def\n",
      "ecto  \n",
      "vicidia\n",
      "l_soun\n",
      "dstore\n",
      ".tgz  \n",
      "TAR \n",
      "Comp\n",
      "rimid\n",
      "o\n",
      "\n",
      "asterisk-rvvvv | grep -e \n",
      "6999  \n",
      "comando para ver la traza solo de una extension  \n",
      " Comandos basicos para identificar problemas con \n",
      "las extensiones que no le ingresan llamadas. \n",
      "Comando  Descripcion  \n",
      "dabatase show CF  Muestra los callforwad creados por medio del \n",
      "comando *72  \n",
      "database show CFU  Muestra los callforwad creados por medio del\n",
      "\n",
      "2  \n",
      " \n",
      "show channels \n",
      "verbose  \n",
      "Muestra los canales activos con el tiempo \n",
      "VERSION 1,4 ASTERISK  \n",
      "Soft hangup channel  Para colgar la llamada VERSION 1,4 ASTERISK  \n",
      "Core show channel  Muestra canales activos APARATIR DE ASTERISK \n",
      "1.6  \n",
      "Channel request \n",
      "hangup  Colgar la llamada APARATIR DE ASTERISK 1.6  \n",
      "asterisk-rvvvv | grep -e \n",
      "6999\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "Chat History: Human: how much is the amount per month the client would need to pay?\n",
      "AI: \n",
      "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
      "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
      "\n",
      "Instructions:\n",
      "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
      "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
      "3. Provide a concise and precise answer.\n",
      "\n",
      "Provided Information\n",
      "-------\n",
      "<ctx>\n",
      "Context: Client: It would, but I don’t know if I’ll have a job by then.\n",
      "Agent: We also offer hardship plans that extend payments over 12 months with no penalties.\n",
      "Client: That sounds better. What would the payments look like?\n",
      "Agent: It would be about $30 per month for 12 months.\n",
      "Client: Okay, but what happens if I miss a payment?\n",
      "\n",
      "Agent: Good afternoon, this is John from Recovery Services. I see your balance of $350 is overdue.\n",
      "Client: Yes, I just lost my job and can’t make a payment right now.\n",
      "Agent: I’m very sorry to hear that. Would a one-month extension help?\n",
      "Client: It would, but I don’t know if I’ll have a job by then.\n",
      "\n",
      "Client: Okay, but what happens if I miss a payment?\n",
      "Agent: If you notify us ahead of time, we can reschedule without late fees.\n",
      "Client: That’s really helpful. Will this affect my credit score?\n",
      "Agent: No, as long as you follow the agreement.\n",
      "Client: I’d like to do that, then.\n",
      "Agent: Great! I’ll send you a hardship agreement for signature today.\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "Chat History: \n",
      "</hs>\n",
      "-------\n",
      "Question: how much is the amount per month the client would need to pay?\n",
      "\n",
      "Useful Answer:\n",
      "The client would need to pay $30 per month for 12 months.\n",
      "</hs>\n",
      "-------\n",
      "Question: what are the basic commands of asterisk?\n",
      "\n",
      "Useful Answer:\n",
      "- `database show CF`: Shows call forward created by *72 command.\n",
      "- `database show CFU`: Shows call forward created by *73 command.\n",
      "- `show channels verbose`: Shows active channels with time.\n",
      "- `soft hangup channel`: Hangs up the call.\n",
      "- `core show channel`: Shows active channels from Asterisk 1.6.\n",
      "- `channel request hangup`: Hangs up the call from Asterisk 1.6.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the basic commands of asterisk?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"you don't gave me all the basic commands, why? the ones you gave me are more important?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'from the solutions guide, give me \"Comandos basicos para verificar el estado de las conferencias\"'\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what does the RED alarm mean?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the agent very sorry to hear?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how can i fix a queue-stats issue?\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
