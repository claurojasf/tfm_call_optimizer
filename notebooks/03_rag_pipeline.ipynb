{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query de usuario, extraer transcripciones relevantes y respuesta de LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from pinecone import Pinecone  # ✅ Correct import for Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Hugging Face API Key (Ensure it's set in your environment variables)\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not HUGGINGFACE_API_KEY:\n",
    "    raise ValueError(\"❌ Missing Hugging Face API Key. Set it as HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# ✅ Load DeepSeek-R1 via API Inference\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",  # Model name\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 2048},\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "# ✅ Initialize Pinecone Client\n",
    "pinecone_client = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# ✅ Ensure the new Pinecone index name is set\n",
    "index_name = os.getenv(\"PINECONE_INDEX_V3\")  # ✅ Updated to use the latest index\n",
    "if not index_name:\n",
    "    raise ValueError(\"❌ PINECONE_INDEX_V3 is not set. Check your .env file.\")\n",
    "\n",
    "# ✅ Ensure the index exists before using it\n",
    "existing_indexes = [idx[\"name\"] for idx in pinecone_client.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    raise ValueError(f\"❌ Index '{index_name}' does not exist in Pinecone. Please create it first.\")\n",
    "\n",
    "# ✅ Initialize Pinecone Index\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# ✅ Initialize the same embedding model used for storing embeddings\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# ✅ Load the vector store for retrieval\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,  # ✅ Connects to the updated Pinecone index\n",
    "    embedding=huggingface_embeddings,\n",
    "    text_key=\"text\"  # ✅ Specifies where text metadata is stored\n",
    ")\n",
    "\n",
    "# ✅ Define the specific call ID for testing\n",
    "test_call_id = \"call_0003\"  # Change this to test different calls\n",
    "\n",
    "# ✅ Create a retriever for call_002 + general PDF solutions\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"filter\": {\n",
    "            \"$or\": [\n",
    "                {\"call_id\": test_call_id},\n",
    "                {\"source\": \"pdf\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Define the Prompt Template for RAG-based Q&A\n",
    "prompt_template = \"\"\"\n",
    "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
    "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
    "\n",
    "Instructions:\n",
    "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
    "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
    "3. Provide a concise and precise answer.\n",
    "4. Quiero las respuestas en idioma español.\n",
    "\n",
    "Provided Information\n",
    "-------\n",
    "<ctx>\n",
    "Context: {context}\n",
    "</ctx>\n",
    "-------\n",
    "<hs>\n",
    "Chat History: {chat_history}\n",
    "</hs>\n",
    "-------\n",
    "Question: {question}\n",
    "\n",
    "Useful Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"]  # ✅ Includes chat history now\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Memory buffer to handle multiple interactions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "# ✅ Define Retrieval-Based Q&A Chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"memory\": memory}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear() # para resetear memoria (si no se hace, se puede superar el limite de tokens y da error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudio\\tfm_call_optimizer\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert auditor analyzing call center transcription calls from custumer support calls.\n",
      "You need to give useful insights from the questions an audit expert would make so he can understand how the calls went.\n",
      "\n",
      "Instructions:\n",
      "1. Answer based on the provided context (delimited by <ctx> </ctx>) and the chat history (delimited by <hs> </hs>) below.\n",
      "2. If the information is not in the context, respond: \"I don't have this information.\"\n",
      "3. Provide a concise and precise answer.\n",
      "4. Quiero las respuestas en idioma español.\n",
      "\n",
      "Provided Information\n",
      "-------\n",
      "<ctx>\n",
      "Context: Sí, en una nueva llamada. ¿Sabe a Amor a Salles? ¿ que Natalia convierte en un gusto? Natalia de Unas Salles con Daniela Rojas. Yo soy una autopatía comunicada. Muy bien, estoy Daniela\n",
      "\n",
      "59  \n",
      " \n",
      "• --archive - Re-indexa las listas archivadas. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vicidial - Procesos de Backup \n",
      "   \n",
      "    \n",
      "          \n",
      "Ir a la navegación   Ir a la búsqueda\n",
      "\n",
      "bases de datos que no son parte de la instalación estándar.  \n",
      "GRABACIONES DE LLAMADAS \n",
      "Puede consultar el documento Vicidial - Archivado de Grabaciones  para obtener \n",
      "información detallado sobre el manejo de Archivos de Grabaciones de Llamadas.  \n",
      "Las grabaciones se almacenan en el filesystem /var/spool/recordings de cada\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "Chat History: \n",
      "</hs>\n",
      "-------\n",
      "Question: dame un resumen de la llamada\n",
      "\n",
      "Useful Answer:\n",
      "\"La llamada fue entre Natalia de Unas Salles y Daniela Rojas. Natalia se identificó como una autopatía comunicada. No tengo más información sobre el contenido de la llamada.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"dame un resumen de la llamada\"\n",
    "response = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
